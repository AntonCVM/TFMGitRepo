# Resultados

En este cap铆tulo se presentan los resultados obtenidos a lo largo de los tres enfoques de entrenamiento implementados. Cada uno representa un aumento progresivo de complejidad en la representaci贸n del entorno y en las se帽ales de recompensa, lo que permite evaluar las limitaciones y ventajas relativas de cada aproximaci贸n.

---

## Enfoque 1: Observaciones globales sin shaping

El agente recib铆a coordenadas globales normalizadas tanto de s铆 mismo como de los objetivos. Sin embargo, la ausencia de shaping y de estructuras que guiasen la atenci贸n provoc贸 una **exploraci贸n muy deficiente**:

* Rara vez abandonaba la habitaci贸n inicial.
* Apenas consegu铆a recolectar objetos.
* El aprendizaje se caracteriz贸 por una **pendiente casi nula** en la evoluci贸n del reward.

 *Figura 1. Evoluci贸n del reward durante el entrenamiento del Enfoque 1.*
*(placeholder para gr谩fica con pendiente baja, casi plana)*

---

## Enfoque 2: Observaciones globales + shaping basado en potencial

La introducci贸n de un shaping de recompensa produjo una **mejora ligera** en la recogida de recolectables e incentiv贸 algo de exploraci贸n adicional. No obstante, tambi茅n aparecieron limitaciones y comportamientos no deseados:

* **Exploits del potencial** debido a cambios abruptos en el spawn y despawn de objetivos.
* Agente que permanec铆a inm贸vil junto a las paredes, recibiendo recompensas sin progresar en la tarea.
* Problemas de granularidad espacial:

  * Con **baldosas grandes**, no era capaz de detectar los pasillos.
  * Con **baldosas peque帽as**, se mov铆a err谩ticamente en c铆rculos.

 *Figura 2. Evoluci贸n del reward durante el entrenamiento del Enfoque 2.*
*(placeholder para gr谩fica con pendiente ligera, mejora lineal muy baja)*

 *Figura 3. Recolectables recogidos en el Enfoque 2 a lo largo del entrenamiento.*
*(placeholder para futura gr谩fica)*

---

## Enfoque 3: Observaciones locales + grafo de se帽ales

El tercer enfoque introdujo un **grafo de propagaci贸n de se帽ales** desde los recolectables, junto con observaciones locales polares. Adem谩s, se aplic贸 shaping basado en r茅cords de aproximaci贸n en lugar de potencial directo.

Los resultados fueron claramente superiores:

* El agente aprendi贸 **r谩pidamente** a navegar habitaciones y pasillos.
* Las se帽ales del grafo guiaron su atenci贸n de forma consistente.
* El shaping por r茅cords **evit贸 los exploits** anteriores.
* Se observ贸 un comportamiento **estable y eficiente**, con una tasa sostenida de progreso hacia los objetivos.
* Tambi茅n mostr贸 la capacidad de **distinguir y esquivar obst谩culos negativos** con su informaci贸n local. Sin embargo, esta evitaci贸n no fue completamente fiable, lo que sugiere que la penalizaci贸n aplicada a los obst谩culos era demasiado baja para consolidar un rechazo perfecto.

 *Figura 4. Evoluci贸n del reward durante el entrenamiento del Enfoque 3.*
*(placeholder para gr谩fica con subida r谩pida inicial y plateau alto a mitad del entrenamiento)*

 *Figura 5. Recolectables recogidos en el Enfoque 3 a lo largo del entrenamiento.*
*(placeholder para futura gr谩fica)*

---

## Comparativa general

Aunque los valores absolutos de reward no son directamente comparables entre enfoques, las gr谩ficas permiten observar tendencias claras:

* **Enfoque 1**: estancamiento casi completo.
* **Enfoque 2**: ligera mejora, pero con comportamientos espurios.
* **Enfoque 3**: avance sostenido y eficiente, con un patr贸n de aprendizaje robusto.
